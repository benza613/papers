# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

Devlin Jacob, Ming-Wei Chang, Kenton Lee and Kristina Toutanova


## Summary 

In “BERT: Pre-training of deep bidirectional transformers for language understanding", Jacob Devlin et all have presented a new approach to language modelling on unlabeled data. 

- The authors present their findings on how BERT can be modelled & trained on sentence prediction & analogy tasks. Basically, the idea put forward is that BERT is a language model that unlike traditional models predicts the next sentences using both left and right context for a given token. They do this by considering not only the previous n-tokens but also the forward n-tokens. 

- The authors implement this approach using a Transformer architecture for encoding the input sentences. They also run a series of experiments to show BERT performs significantly better when provided a much larger number of input parameters. The authors finally evaluated their findings on a number of NLP tasks to demonstrate state of the art result scores.  

## Thoughts on Strength / Weaknesses 

- One of the key strengths of this paper is that it focuses on training a deep language model in order to generate vectors that are based on the surrounding context. 

- Traditionally, word embeddings generated by word2vec or glove were mapped to vectors that characterized some linguistic aspect of that word in the training corpora. However, these traditional vector embeddings have limitations of what they can capture since the training is quite shallow & they do not take context into account here. 

- Another key strength could be the use of a transformer architecture for attention instead of a more mainstream LSTM. The authors have used a deep learning approach here to basically train the model to learn how to use context from the neighborhood to predict the missing token. 

- While this paper doesn’t exhibit any significant points of concern, I would wonder about how the percentage of masking input tokens would affect the overall model accuracy and training. It is possible that masking too much may cause context to be affected while masking too less could possibly affect training accuracy and time. The authors haven’t discussed any variation here and have mostly fixated on a fixed masking rate of 15%.

## Thoughts on Analysis / Evaluation

- The methodology put forward by BERT is basically that of predicting the probability of a word occurring in the particular context. The authors setup the language model to use the transformer architecture instead of an LSTM for the attention modelling capability. I definitely feel that this choice is good since a LSTM model, even a bi-directional one, does not take left and right context into account at the SAME time; since an LSTM will first train left to right, then in reverse, but not simultaneously. 

- I feel this contextual approach is what drives BERT to have much superior accuracy scores. The authors finally trained & fine-tuned models using a number of hyperparameter configurations. This fine tuning was required as some settings were generally found to work better for different tasks. 

- The authors have also evaluated BERT against a number of well-established NLP tasks like GLUE, SQUAD, SWAG & studied the effects that pre-training & model sizes on the prediction scores. 


_Ultimately, I feel that this paper has definitely shown tremendous improvements over the established standard language modelling embeddings available. BERT’s ability to handle context over a whole sentence definitely has been the main reason for achieving state of the art results. I think that authors have definitely presented their claims & results in a concise manner and also provided various implementation details & results in the appendix from the number of experiments they conducted._



#### Citation 

Devlin Jacob, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” NAACL-HLT (2019). https://arxiv.org/pdf/1810.04805.pdf